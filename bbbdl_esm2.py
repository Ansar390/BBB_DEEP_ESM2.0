# -*- coding: utf-8 -*-
"""BBBDL_ESM5_roc_curve.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LvmhL-ZPgzCrb0rH3XYjKMSDFGCtO-Cm
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import SimpleRNN, Dense, Dropout, LSTM, GRU
from tensorflow import keras
from sklearn.metrics import accuracy_score, matthews_corrcoef, precision_score, recall_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout

pip install keras-tuner

import numpy as np
train=pd.read_csv("/content/train_data77up.csv")
test=pd.read_csv("/content/test_data23up.csv")

X_train=train.drop(["target"],axis=1)
y_train=train["target"]

X_test=test.drop(["target"],axis=1)
y_test=test["target"]

df=pd.concat([train,test],axis=0)
y=df["target"]
X=df.drop(["target"],axis=1)

from sklearn.preprocessing import StandardScaler
import numpy as np
# from sklearn.preprocessing import MinMaxScaler
# std_scaletr = StandardScaler().fit(X_train)
# X_train = std_scaletr.fit_transform(X_train)
# X_train = np.nan_to_num(X_train.astype('float32'))

# std_scalets = StandardScaler().fit(X_test)
# X_test = std_scalets.fit_transform(X_test)
# X_test = np.nan_to_num(X_test.astype('float32'))


std_scale = StandardScaler().fit(X)
X = std_scale.fit_transform(X)
X = np.nan_to_num(X.astype('float32'))



input_dim=153
num_classes=2
# X_train = X_train.values.reshape(-1, input_dim)
# X_test = X_test.values.reshape(-1, input_dim)

# # Convert output data to one-hot encoding
y = tf.keras.utils.to_categorical(y, num_classes)
# y_test = tf.keras.utils.to_categorical(y_test, num_classes)

X.shape



# y = y[:, 0]

# unique_values, counts = np.unique(y, return_counts=True)

# # Print unique values and their counts
# print("Unique values:", unique_values)
# print("Counts:", counts)

# # Print the length of unique values
# print("Length of unique values:", len(unique_values))

y.ndim

from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.preprocessing import MinMaxScaler

std_scale = StandardScaler().fit(X)
X = std_scale.fit_transform(X)
X = np.nan_to_num(X.astype('float32'))

"""**LSTM 5 CV**"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping
from sklearn.metrics import matthews_corrcoef
# Define the number of folds for cross-validation
n_splits = 5
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Reshape the data into the required format for LSTM
    timesteps = 1
    features = 153
    X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    X_test = X_test.reshape(X_test.shape[0], timesteps, features)

    # Define the LSTM model architecture with multiple hidden layers
    model = Sequential()
    model.add(LSTM(128, input_shape=(timesteps, features), return_sequences=True))  # First LSTM layer with return_sequences=True
    model.add(LSTM(64, return_sequences=True))  # Second LSTM layer with return_sequences=True
    model.add(LSTM(32, return_sequences=True))  # Third LSTM layer with return_sequences=True
    model.add(LSTM(16, return_sequences=True))  # Fourth LSTM layer with return_sequences=True
    model.add(LSTM(8))  # Fifth LSTM layer without return_sequences=True
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    # Define early stopping callback based on validation accuracy
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)


    # Train the model
    history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=True)

    # Evaluate the model on the testing data
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

    # Predict labels for the test data
    y_pred = model.predict(X_test)
    y_pred_binary = np.round(y_pred)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred_binary)

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
    mcc = matthews_corrcoef(y_test, y_pred_binary)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    # Calculate ROC curve and AUC
    lstm_fpr, lstm_tpr, thresholds = roc_curve(y_test, y_pred)
    lstm_roc_auc = auc(lstm_fpr, lstm_tpr)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(lstm_roc_auc)
    accuracy_scores.append(test_acc)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score





"""**SImple RNN**"""



# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense
from keras.callbacks import EarlyStopping
from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix, roc_curve, auc

# Define the number of folds for cross-validation
n_splits = 5
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Reshape the data into the required format for SimpleRNN
    timesteps = 1
    features = 153
    X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    X_test = X_test.reshape(X_test.shape[0], timesteps, features)

    # Define the SimpleRNN model architecture with multiple hidden layers
    model = Sequential()
    model.add(SimpleRNN(128, input_shape=(timesteps, features), return_sequences=True))  # First SimpleRNN layer with return_sequences=True
    model.add(SimpleRNN(64, return_sequences=True))  # Second SimpleRNN layer with return_sequences=True
    model.add(SimpleRNN(32, return_sequences=True))  # Third SimpleRNN layer with return_sequences=True
    model.add(SimpleRNN(16, return_sequences=True))  # Fourth SimpleRNN layer with return_sequences=True
    model.add(SimpleRNN(8))  # Fifth SimpleRNN layer without return_sequences=True
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)

    # Train the model
    history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=True)

    # Evaluate the model on the testing data
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

    # Predict labels for the test data
    y_pred = model.predict(X_test)
    y_pred_binary = np.round(y_pred)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred_binary)

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
    mcc = matthews_corrcoef(y_test, y_pred_binary)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    # Calculate ROC curve and AUC
    rnn_fpr, rnn_tpr, thresholds = roc_curve(y_test, y_pred)
    rnn_roc_auc = auc(rnn_fpr, rnn_tpr)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(rnn_roc_auc)
    accuracy_scores.append(test_acc)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score



"""**GRU**"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.metrics import f1_score, confusion_matrix, matthews_corrcoef, roc_curve, auc
from sklearn.model_selection import KFold
from keras.models import Sequential
from keras.layers import GRU, Dense
from keras.callbacks import EarlyStopping

# Define the number of folds for cross-validation
n_splits = 5
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Reshape the data into the required format for GRU
    timesteps = 1
    features = 153
    X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    X_test = X_test.reshape(X_test.shape[0], timesteps, features)

    # Define the GRU model architecture with multiple hidden layers
    model = Sequential()
    model.add(GRU(128, input_shape=(timesteps, features), return_sequences=True))  # First GRU layer with return_sequences=True
    model.add(GRU(64, return_sequences=True))  # Second GRU layer with return_sequences=True
    model.add(GRU(32, return_sequences=True))  # Third GRU layer with return_sequences=True
    model.add(GRU(16, return_sequences=True))  # Fourth GRU layer with return_sequences=True
    model.add(GRU(8))  # Fifth GRU layer without return_sequences=True
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)


    # Train the model
    history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=True)

    # Evaluate the model on the testing data
    y_pred = model.predict(X_test)
    y_pred_binary = np.round(y_pred)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred_binary)

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
    mcc = matthews_corrcoef(y_test, y_pred_binary)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    # Calculate ROC curve and AUC
    gru_fpr, gru_tpr, thresholds = roc_curve(y_test, y_pred)
    gru_roc_auc = auc(gru_fpr, gru_tpr)

    # Calculate accuracy
    accuracy = (tp + tn) / (tp + tn + fp + fn)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(gru_roc_auc)
    accuracy_scores.append(accuracy)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score



"""**ANN or FCN**"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix, roc_curve, auc
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Define the number of folds for cross-validation
n_splits = 5
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Reshape the data into the required format for ANN
    X_train = X_train.reshape(X_train.shape[0], -1)
    X_test = X_test.reshape(X_test.shape[0], -1)

    # Define the ANN model architecture with multiple hidden layers
    model = Sequential()
    model.add(Dense(128, input_dim=X.shape[1], activation='relu'))  # First Dense layer
    model.add(Dense(64, activation='relu'))  # Second Dense layer
    model.add(Dense(32, activation='relu'))  # Third Dense layer
    model.add(Dense(16, activation='relu'))  # Fourth Dense layer
    model.add(Dense(8, activation='relu'))  # Fifth Dense layer
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)

    # Train the model
    history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=0)

    # Evaluate the model on the testing data
    y_pred = model.predict(X_test)
    y_pred_binary = np.round(y_pred)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred_binary)

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
    mcc = matthews_corrcoef(y_test, y_pred_binary)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    # Calculate ROC curve and AUC
    ann_fpr, ann_tpr, thresholds = roc_curve(y_test, y_pred)
    ann_roc_auc = auc(ann_fpr, ann_tpr)

    # Calculate accuracy
    accuracy = (tp + tn) / (tp + tn + fp + fn)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(ann_roc_auc)
    accuracy_scores.append(accuracy)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score



"""**CNN**"""

import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from sklearn.model_selection import KFold
from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc

# Define the number of folds for cross-validation
n_splits = 5
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Reshape the data for Conv1D model
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

    # Define the Conv1D model architecture with multiple hidden layers
    model = Sequential()
    model.add(Conv1D(64, 3, activation='relu', input_shape=(X.shape[1], 1)))  # First Conv1D layer
    model.add(MaxPooling1D(2))
    model.add(Conv1D(32, 3, activation='relu'))  # Second Conv1D layer
    model.add(MaxPooling1D(2))
    model.add(Conv1D(16, 3, activation='relu'))  # Third Conv1D layer
    model.add(MaxPooling1D(2))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))  # First Dense layer
    model.add(Dense(32, activation='relu'))  # Second Dense layer
    model.add(Dense(1, activation='sigmoid'))  # Output layer

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)

    # Train the model
    model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=True)

    # Evaluate the model on the testing data
    y_pred = model.predict(X_test)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred.round())

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred.round()).ravel()
    specificity = tn / (tn + fp)
    sensitivity = tp / (tp + fn)
    mcc = ((tp * tn) - (fp * fn)) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

    # Calculate accuracy
    accuracy = (tp + tn) / (tp + tn + fp + fn)

    # Calculate ROC curve and AUC
    cnn_fpr, cnn_tpr, thresholds = roc_curve(y_test, y_pred)
    cnn_roc_auc = auc(cnn_fpr, cnn_tpr)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(cnn_roc_auc)
    accuracy_scores.append(accuracy)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score



"""**Bi LSTM**"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping
from sklearn.metrics import matthews_corrcoef
# Define the number of folds for cross-validation
n_splits = 5
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
f1_scores = []
mcc_scores = []
sensitivity_scores = []
specificity_scores = []
roc_auc_scores = []
accuracy_scores = []  # Add accuracy_scores list

# Perform 5-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Reshape the data into the required format for LSTM
    timesteps = 1
    features = 153
    X_train = X_train.reshape(X_train.shape[0], timesteps, features)
    X_test = X_test.reshape(X_test.shape[0], timesteps, features)

    # Define the LSTM model architecture with multiple hidden layers
    model = Sequential()
    model.add(LSTM(128, input_shape=(timesteps, features), return_sequences=True))  # First LSTM layer with return_sequences=True
    model.add(LSTM(64, return_sequences=True))  # Second LSTM layer with return_sequences=True
    model.add(LSTM(32, return_sequences=True))  # Third LSTM layer with return_sequences=True
    model.add(LSTM(16, return_sequences=True))  # Fourth LSTM layer with return_sequences=True
    model.add(LSTM(8))  # Fifth LSTM layer without return_sequences=True
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Define early stopping callback
    # early_stop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)
    # Define early stopping callback based on validation accuracy
    early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='max', verbose=1, restore_best_weights=True)


    # Train the model
    history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop], verbose=True)

    # Evaluate the model on the testing data
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

    # Predict labels for the test data
    y_pred = model.predict(X_test)
    y_pred_binary = np.round(y_pred)

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred_binary)

    # Calculate additional evaluation metrics
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
    mcc = matthews_corrcoef(y_test, y_pred_binary)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)

    # Calculate ROC curve and AUC
    bi_fpr, bi_tpr, thresholds = roc_curve(y_test, y_pred)
    bi_roc_auc = auc(bi_fpr, bi_tpr)

    # Append scores to lists
    f1_scores.append(f1)
    mcc_scores.append(mcc)
    sensitivity_scores.append(sensitivity)
    specificity_scores.append(specificity)
    roc_auc_scores.append(lstm_roc_auc)
    accuracy_scores.append(test_acc)  # Add accuracy score

# Calculate average scores across folds
avg_f1_score = np.mean(f1_scores)
avg_mcc_score = np.mean(mcc_scores)
avg_sensitivity_score = np.mean(sensitivity_scores)
avg_specificity_score = np.mean(specificity_scores)
avg_roc_auc_score = np.mean(roc_auc_scores)
avg_accuracy_score = np.mean(accuracy_scores)  # Calculate average accuracy score

# Print average scores
print("Average F1 Score across folds:", avg_f1_score)
print("Average MCC across folds:", avg_mcc_score)
print("Average Sensitivity across folds:", avg_sensitivity_score)
print("Average Specificity across folds:", avg_specificity_score)
print("Average ROC AUC across folds:", avg_roc_auc_score)
print("Average Accuracy across folds:", avg_accuracy_score)  # Print average accuracy score

# replace X1 with X_test and Y1 with y_test
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt



plt.figure(figsize=(20, 10), dpi=600)
plt.plot([0, 1], [0, 1], linestyle="--", lw=2,  label="Chance", alpha=0.8)
plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM (auc = %0.3f)' % lstm_roc_auc)
plt.plot(rnn_fpr, rnn_tpr, marker='.', label='RNN (auc = %0.3f)' % rnn_roc_auc)
plt.plot(gru_fpr, gru_tpr, linestyle='-', label='GRU (auc = %0.3f)' % gru_roc_auc)
plt.plot(ann_fpr, ann_tpr, linestyle='-', label='FCN (auc = %0.3f)' % ann_roc_auc)
plt.plot(cnn_fpr, cnn_tpr, linestyle='-', label='CNN (auc = %0.3f)' % cnn_roc_auc)
plt.plot(bi_fpr, bi_tpr, linestyle='-', label='Bi-LSTM (auc = %0.3f)' % bi_roc_auc)


# plt.xlabel('False Positive Rate -->')
# plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=20, ncol=1)

plt.show()











# Reshape input data
from sklearn.metrics import confusion_matrix
import tensorflow as tf
X_train = X_train.reshape(-1, input_dim)
X_test = X_test.reshape(-1, input_dim)

# # Convert output data to one-hot encoding
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

# Reshape input data
from sklearn.metrics import confusion_matrix
import tensorflow as tf
# Reshape input data
timestamp=1
X_train = X_train.reshape(X_train.shape[0], timestamp, X_train.shape[1])

X_train.shape

from keras.models import Sequential
from keras.layers import SimpleRNN, Dense, Dropout

num_timesteps = 1
num_features = 153

rnn = Sequential()
rnn.add(SimpleRNN(units=128, input_shape=(num_timesteps, num_features)))
rnn.add(Dropout(0.5))  # Add dropout layer for regularization
rnn.add(Dense(units=64, activation='relu'))  # Add additional dense layer
rnn.add(Dropout(0.5))  # Add dropout layer for regularization
rnn.add(Dense(units=2, activation='softmax'))

rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
from keras.callbacks import ModelCheckpoint

rnncheckpoint = ModelCheckpoint('best_modelrnn1.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

rnn.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),callbacks=[rnncheckpoint])

from tensorflow import keras
from sklearn.metrics import accuracy_score, matthews_corrcoef, precision_score, recall_score


best_modellstm = keras.models.load_model('best_modelrnn1.h5')
predictions = rnn.predict(X_test)


# Get the predictions from the best model
y_test_pred = best_modellstm.predict(X_test)

# Convert the predictions to binary labels
y_test_pred_labels = np.argmax(y_test_pred, axis=1)

# Convert the true labels to binary labels
y_test_true_labels = np.argmax(y_test, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_true_labels, y_test_pred_labels)

# Calculate MCC
test_mcc = matthews_corrcoef(y_test_true_labels, y_test_pred_labels)

# Calculate specificity
test_specificity = recall_score(y_test_true_labels, y_test_pred_labels, pos_label=0)

# Calculate sensitivity
test_sensitivity = recall_score(y_test_true_labels, y_test_pred_labels, pos_label=1)
print("LSTM")
print("Test Accuracy:", test_accuracy)
print("Test MCC:", test_mcc)
print("Test Specificity:", test_specificity)
print("Test Sensitivity:", test_sensitivity)

from keras.models import Sequential
from keras.layers import Bidirectional, LSTM, Dense, Dropout

num_timesteps = 1
num_features = 153

bi_lstm = Sequential()
bi_lstm.add(Bidirectional(LSTM(units=128, return_sequences=False), input_shape=(num_timesteps, num_features)))
bi_lstm.add(Dropout(0.5))  # Add dropout layer for regularization
bi_lstm.add(Dense(units=64, activation='relu'))  # Add additional dense layer
bi_lstm.add(Dropout(0.5))  # Add dropout layer for regularization
bi_lstm.add(Dense(units=2, activation='softmax'))

bi_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
from keras.callbacks import ModelCheckpoint

bilstmcheckpoint = ModelCheckpoint('best_modelbilstm.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

bi_lstm.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),callbacks=[bilstmcheckpoint])

from tensorflow import keras
from sklearn.metrics import accuracy_score, matthews_corrcoef, precision_score, recall_score


best_modellstm = keras.models.load_model('best_modelbilstm.h5')
predictions = bi_lstm.predict(X_test)


# Get the predictions from the best model
y_test_pred = best_modellstm.predict(X_test)

# Convert the predictions to binary labels
y_test_pred_labels = np.argmax(y_test_pred, axis=1)

# Convert the true labels to binary labels
y_test_true_labels = np.argmax(y_test, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_true_labels, y_test_pred_labels)

# Calculate MCC
test_mcc = matthews_corrcoef(y_test_true_labels, y_test_pred_labels)

# Calculate specificity
test_specificity = recall_score(y_test_true_labels, y_test_pred_labels, pos_label=0)

# Calculate sensitivity
test_sensitivity = recall_score(y_test_true_labels, y_test_pred_labels, pos_label=1)
print("LSTM")
print("Test Accuracy:", test_accuracy)
print("Test MCC:", test_mcc)
print("Test Specificity:", test_specificity)
print("Test Sensitivity:", test_sensitivity)



from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/bbbDL

import keras
fcn = keras.models.load_model('best_modelfcn.h5')
cnn = keras.models.load_model('best_modelccn.h5')
lstm = keras.models.load_model('best_modellstm1.h5')
gru = keras.models.load_model('best_modelgru1.h5')
rnn = keras.models.load_model('best_modelrnn1.h5')
bi = keras.models.load_model('best_modelbilstm.h5')

X_train = np.load('X_trainbbb.npy')
y_train = np.load('y_trainbbb.npy')
X_test = np.load('X_testbbb.npy')
y_test = np.load('y_testbbb.npy')

testx = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])
trainx = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])

y_test_true_labels = np.argmax(y_test, axis=1)
# y_test_true_labels

fcn_pred_prob = fcn.predict(X_test)[:, 1]
cnn_pred_prob = cnn.predict(X_test)[:, 1]
lstm_pred_prob = lstm.predict(testx)[:, 1]
rnn_pred_prob = rnn.predict(testx)[:, 1]
bi_pred_prob = bi.predict(testx)[:, 1]
fru_pred_prob = gru.predict(testx)[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
fcn_fpr, fcn_tpr, fcn_thresholds = roc_curve(y_test_true_labels, fcn_pred_prob)
cnn_fpr, cnn_tpr, cnn_thresholds = roc_curve(y_test_true_labels, cnn_pred_prob)
lstm_fpr, lstm_tpr, lstm_thresholds = roc_curve(y_test_true_labels, lstm_pred_prob)
bi_fpr, bi_tpr, bi_thresholds = roc_curve(y_test_true_labels, bi_pred_prob)
rnn_fpr, rnn_tpr, rnn_thresholds = roc_curve(y_test_true_labels, rnn_pred_prob)
fru_fpr, fru_tpr, fru_thresholds = roc_curve(y_test_true_labels, fru_pred_prob)

fcn_auc = roc_auc_score(y_test_true_labels, fcn_pred_prob)
cnn_auc = roc_auc_score(y_test_true_labels, cnn_pred_prob)
lstm_auc = roc_auc_score(y_test_true_labels, lstm_pred_prob)
fru_auc = roc_auc_score(y_test_true_labels, fru_pred_prob)
rnn_auc = roc_auc_score(y_test_true_labels, rnn_pred_prob)
bi_auc = roc_auc_score(y_test_true_labels, bi_pred_prob)

# Compute ROC AUC score
roc_auc = roc_auc_score(test_labels, predicted_labels)

# Compute ROC curve
fpr, tpr, _ = roc_curve(test_labels, predicted_labels)

plt.figure(figsize=(20, 10), dpi=600)
plt.plot([0, 1], [0, 1], linestyle="--", lw=2,  label="Chance", alpha=0.8)
# Plot the ROC curve for each model
plt.plot(fcn_fpr, fcn_tpr, marker='.', label='FCN (auc = %0.3f)' % fcn_auc)
plt.plot(cnn_fpr, cnn_tpr, linestyle='-', label='CNN (auc = %0.3f)' % cnn_auc)
plt.plot(rnn_fpr, rnn_tpr, linestyle='-', label='RNN (auc = %0.3f)' % rnn_auc)
plt.plot(lstm_fpr, lstm_tpr, linestyle='-', label='LSTM (auc = %0.3f)' % lstm_auc)
plt.plot(bi_fpr, bi_tpr, linestyle='-', label='Bi-LSTM (auc = %0.3f)' % bi_auc)
plt.plot(fru_fpr, fru_tpr, linestyle='-', label='GRU (auc = %0.3f)' % fru_auc)
plt.plot(fpr, tpr, linestyle='-', label='ESM Fine Tuned (auc = %0.3f)' % roc_auc)


# plt.plot(fcn_fpr, fcn_tpr, label=f'FCN (AUC = {fcn_auc:.4f})')
# plt.plot(cnn_fpr, cnn_tpr, label=f'CNN (AUC = {cnn_auc:.4f})')
# plt.plot(lstm_fpr, lstm_tpr, label=f'LSTM (AUC = {lstm_auc:.4f})')
# plt.plot(fru_fpr, fru_tpr, label=f'GRU (AUC = {fru_auc:.4f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line representing random classifier
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend()
plt.legend(loc="lower right", fontsize=20, ncol=1)

plt.show()

pip install Bio datasets

pip install transformers[torch]





from Bio import SeqIO
import pandas as pd

def fasta_sequences_to_dataframe(fasta_file):
    sequences = []

    # Parse the FASTA file
    for record in SeqIO.parse(fasta_file, "fasta"):
        sequences.append(str(record.seq))

    # Create a DataFrame
    df = pd.DataFrame({'sequence': sequences})

    return df

# Specify input FASTA file
fasta_file = "/content/negbbb.fasta"  # Replace with the path to your FASTA file

# Convert FASTA sequences to DataFrame
neg = fasta_sequences_to_dataframe(fasta_file)

# Print the DataFrame
neg["class"]=0

from Bio import SeqIO
import pandas as pd

def fasta_sequences_to_dataframe(fasta_file):
    sequences = []

    # Parse the FASTA file
    for record in SeqIO.parse(fasta_file, "fasta"):
        sequences.append(str(record.seq))

    # Create a DataFrame
    df = pd.DataFrame({'sequence': sequences})

    return df

# Specify input FASTA file
fasta_file = "/content/posbbb.fasta"  # Replace with the path to your FASTA file

# Convert FASTA sequences to DataFrame
pos = fasta_sequences_to_dataframe(fasta_file)

# Print the DataFrame
pos["class"]=1

df= pd.concat([pos,neg],axis=0)

X=df.drop(["class"],axis=1)

sequences = X["sequence"].tolist()
labels= df["class"].tolist()

model_checkpoint = "facebook/esm2_t12_35M_UR50D"

from sklearn.model_selection import train_test_split

train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.27, shuffle=True)

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

tokenizer(train_sequences[0])

train_tokenized = tokenizer(train_sequences)
test_tokenized = tokenizer(test_sequences)

from datasets import Dataset
train_dataset = Dataset.from_dict(train_tokenized)
test_dataset = Dataset.from_dict(test_tokenized)

train_dataset

train_dataset = train_dataset.add_column("labels", train_labels)
test_dataset = test_dataset.add_column("labels", test_labels)
train_dataset

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

num_labels = max(train_labels + test_labels) + 1  # Add 1 since 0 can be a label
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)

model_name = model_checkpoint.split("/")[-1]
batch_size = 8

args = TrainingArguments(
    f"{model_name}-BBBPeptides-finetuned",
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=10,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    # push_to_hub=True,
)

from evaluate import load
import numpy as np

metric = load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)

# pip install huggingface-hub

from huggingface_hub import login

# Your Hugging Face API key
api_key = "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"

# Login to the Hugging Face model hub
login(api_key)

# Check if login was successful
if api_key:
    print("Login successful!")
else:
    print("Login failed. Please check your API key.")



trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

from sklearn.metrics import matthews_corrcoef, confusion_matrix, roc_auc_score, roc_curve, recall_score
predictions = trainer.predict(test_dataset)

# Extract the predicted labels from the output of predict()
predicted_labels = np.argmax(predictions.predictions, axis=1)

len(predictions)

from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

mcc = accuracy_score(test_labels, predicted_labels)
mcc

# Compute confusion matrix
tn, fp, fn, tp = confusion_matrix(test_labels, predicted_labels).ravel()

# Compute MCC
mcc = matthews_corrcoef(test_labels, predicted_labels)

# Compute specificity
specificity = tn / (tn + fp)

# Compute sensitivity (recall)
sensitivity = recall_score(test_labels, predicted_labels)

# Compute ROC AUC score
roc_auc = roc_auc_score(test_labels, predicted_labels)

# Compute ROC curve
fpr, tpr, _ = roc_curve(test_labels, predicted_labels)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# Print computed metrics
print("Matthews Correlation Coefficient (MCC):", mcc)
print("Specificity:", specificity)
print("Sensitivity (Recall):", sensitivity)
print("ROC AUC Score:", roc_auc)



from evaluate import load
import numpy as np
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, matthews_corrcoef, recall_score

# Load accuracy metric
metric_accuracy = load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    # Compute metrics
    mcc = matthews_corrcoef(labels, predictions)
    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()
    specificity = tn / (tn + fp)
    sensitivity = recall_score(labels, predictions)
    accuracy = metric_accuracy.compute(predictions=predictions, references=labels)
    roc_auc = roc_auc_score(labels, predictions)

    # Compute ROC curve
    fpr, tpr, _ = roc_curve(labels, predictions)

    # Convert NumPy arrays to lists
    fpr = fpr.tolist()
    tpr = tpr.tolist()

    return {
        "mcc": mcc,
        "specificity": specificity,
        "sensitivity": sensitivity,
        "accuracy": accuracy,
        "roc_auc": roc_auc,
        "roc_curve": (fpr, tpr)
    }

# Pass the compute_metrics function to Trainer
trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()



len(predictions)

# predictions = np.argmax(predictions.predictions, axis=1)
predictions = trainer.predict(test_dataset)
# Compute any additional evaluation metrics or analysis you require
# For example, you can compute a confusion matrix
conf_matrix = confusion_matrix(test_labels, predictions)
conf_matrix

